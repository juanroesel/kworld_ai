{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Conversational Agent\n",
    "\n",
    "This notebook provides a prototype of the Conversational Agent, leveraging the in-memory embedding and vector storage capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getenv(\"VECTOR_DB_DIR\")\n",
    "\n",
    "# Insantiates Embedding model\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    ")\n",
    "\n",
    "# Loads DB from disk\n",
    "vectordb = Chroma(\n",
    "    persist_directory=data_path,\n",
    "    embedding_function=embedder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(),\n",
    "    vectordb.as_retriever(search_kwargs={'k': 6}),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What services does KPMG offer?\n",
      "Answer: KPMG offers a wide range of professional services including audit, tax, and advisory services. They provide services in areas such as financial statement audits, tax planning and compliance, risk management, management consulting, and more. For more detailed information about their services, please visit their official website at https://kpmg.com.\n",
      "\n",
      "Exiting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanr\\Documents\\Profesional\\KPMG\\technical_case\\kworld_ir_chatbot\\kworld_ai\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    # this prints to the terminal, and waits to accept an input from the user\n",
    "    query = input('Insert query: ')\n",
    "    print()\n",
    "    # give us a way to exit the script\n",
    "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
    "        print('Exiting')\n",
    "        sys.exit()\n",
    "    # we pass in the query to the LLM, and print out the response. As well as\n",
    "    # our query, the context of semantically relevant information from our\n",
    "    # vector store will be passed in, as well as list of our chat history\n",
    "    print('Query: ', query)\n",
    "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'])\n",
    "    # we build up the chat_history list, based on our question and response\n",
    "    # from the LLM, and the script then returns to the start of the loop\n",
    "    # and is again ready to accept user input.\n",
    "    chat_history.append((query, result['answer']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA and Similarity Search (Debugger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, KPMG has offices in Canada.\n",
      "[Document(page_content='KPMG in Canada', metadata={'source': 'https://kpmg.com/xx/en/home/industries/healthcare/covid-19-and-healthcare/connected-health/drinking-from-the-fire-hose.html'}), Document(page_content='KPMG in Canada', metadata={'source': 'https://kpmg.com/xx/en/home/industries/healthcare/covid-19-and-healthcare/connected-health/drinking-from-the-fire-hose.html'}), Document(page_content='KPMG in Canada', metadata={'source': 'https://kpmg.com/xx/en/home/industries/healthcare/covid-19-and-healthcare/connected-health/drinking-from-the-fire-hose.html'}), Document(page_content='KPMG in Canada', metadata={'source': 'https://kpmg.com/xx/en/home/industries/healthcare/covid-19-and-healthcare/connected-health/drinking-from-the-fire-hose.html'})]\n"
     ]
    }
   ],
   "source": [
    "#query = \"What services does KPMG offer?\"\n",
    "query = \"Does KPMG have offices in Canada?\"\n",
    "#query = \"Get me a recipe for a lemon cupcake\"\n",
    "#query = \"Who are KPMG's strategic alliance partners in technology?\"\n",
    "#query = \"Who is Bill Thomas? What are his qualifications?\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 7}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain({'query': query})\n",
    "print(result['result'])\n",
    "\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
