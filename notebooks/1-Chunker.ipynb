{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import bs4\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOUP_OUT_DIR = os.getenv(\"SOUP_OUT_DIR\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\")\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Dataset contains the mapping between source (i.e., the website) and \n",
    "    its corresponding chunks of text, extracted through the Chunker pipeline.\n",
    "    \"\"\"\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super().__new__(cls)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = defaultdict(list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        return self.__dict__\n",
    "    \n",
    "    def __setstate__(self, data):\n",
    "        self.__dict__ = data\n",
    "\n",
    "    def add_data(self, source: str, chunks: List[langchain.schema.document.Document]):\n",
    "        if not isinstance(source, str) and isinstance(chunks, list):\n",
    "            raise TypeError(\"Make sure 'source' and 'chunks' are in the right format\")\n",
    "        self.data[source].extend(chunks)\n",
    "    \n",
    "    def get_chunks(self, source: str):\n",
    "        return self.data.get(source, None)\n",
    "\n",
    "\n",
    "def load_soup(dir_path: str):\n",
    "    \"\"\"\n",
    "    Yields one soup at a time using a generator.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Path to the directory containing the pickled soups\n",
    "    \"\"\"\n",
    "\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            with open(os.path.abspath(os.path.join(dir_path, file)), \"rb\") as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "def extract_url(soup: bs4.BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Extracts the URL that contains the information to be extracted.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): bs4 object\n",
    "\n",
    "    Returns:\n",
    "        str | None: string with desired URL, None if not found\n",
    "    \"\"\"\n",
    "    for link in soup.find_all(\"link\"):\n",
    "        href = link.attrs.get(\"href\")\n",
    "        if href and BASE_URL and BASE_URL in href:\n",
    "            return href\n",
    "    return\n",
    "\n",
    "def extract_paragraph_text(paragraph: bs4.element.Tag):\n",
    "    \"\"\"\n",
    "    Extracts the textual information from a specific <p> tag in a given HTML page.\n",
    "\n",
    "    Args:\n",
    "        paragraph (bs4.element.Tag): bs4 object extracted from the <p> tag\n",
    "\n",
    "    Returns:\n",
    "        str | None: Extracted string or None\n",
    "    \"\"\"\n",
    "    if isinstance(paragraph, bs4.element.Tag):\n",
    "        return paragraph.get_text()\n",
    "    return\n",
    "\n",
    "def chunk_text(\n",
    "        text_splitter: RecursiveCharacterTextSplitter, \n",
    "        paragraph_text: str, \n",
    "        source: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Partitions a given paragraph text into specific chunks.\n",
    "\n",
    "    Args:\n",
    "        text_splitter (RecursiveCharacterTextSplitter): Text splitter object provided by Langchain\n",
    "        paragraph_text (str): Text extracted from a <p> tag\n",
    "        source (str): URL extracted from a given soup object\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, langchain.schema.document.Document | str]]: Inverted index mapping source with a list of chunks\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = text_splitter.create_documents(\n",
    "        texts=[paragraph_text], \n",
    "        metadatas=[{\"source\": source}]\n",
    "    )\n",
    "    return [{\"text\": chunk.page_content, \"source\": source} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64429it [40:25, 26.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate text splitter\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Instantiate Dataset\n",
    "dataset = Dataset()\n",
    "\n",
    "# Iterate over processed soups and extract the relevant data\n",
    "i = 0\n",
    "for soup in tqdm(load_soup(SOUP_OUT_DIR)):\n",
    "    url = extract_url(soup)\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        text = extract_paragraph_text(p)\n",
    "        \n",
    "        if url and text:\n",
    "            chunks = text_splitter.create_documents(\n",
    "                texts=[text], \n",
    "                metadatas=[{\"source\": url}]\n",
    "            )\n",
    "\n",
    "            if chunks:\n",
    "                dataset.add_data(url, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4491"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.getenv(\"CHUNK_OUT_DIR\")\n",
    "\n",
    "with open(out_path + \"kworld_chunked_dataset.pkl\", \"wb\") as pickle_file:\n",
    "    pickle.dump(dataset, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load persisted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/chunked/kworld_chunked_dataset.pkl\"\n",
    "\n",
    "with open(data_path, \"rb\") as pickle_file:\n",
    "  dataset = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2727267"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_dataset = [chunk for _, chunks in dataset.data.items() for chunk in chunks]\n",
    "len(flattened_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    # model_kwargs={\"device\": \"cuda\"},\n",
    "    # encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "  flattened_dataset,\n",
    "  embedding=embedder,\n",
    "  persist_directory='../data/vector_db/'\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\juanr\\Documents\\Profesional\\KPMG\\technical_case\\kworld_ir_chatbot\\kworld_ai\\notebooks\\1-chunker.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m qa_chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     llm\u001b[39m=\u001b[39mOpenAI(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     retriever\u001b[39m=\u001b[39mvectordb\u001b[39m.\u001b[39mas_retriever(search_kwargs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m7\u001b[39m}),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat services does KPMG offer?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#query = \"Does KPMG have offices in Canada?\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/juanr/Documents/Profesional/KPMG/technical_case/kworld_ir_chatbot/kworld_ai/notebooks/1-chunker.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#query = \"Did FB Barcelona win yesterday?\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RetrievalQA' is not defined"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 7}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "query = \"What services does KPMG offer?\"\n",
    "#query = \"Does KPMG have offices in Canada?\"\n",
    "#query = \"Did FB Barcelona win yesterday?\"\n",
    "\n",
    "result = qa_chain({'query': query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
